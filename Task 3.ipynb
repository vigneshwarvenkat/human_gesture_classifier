{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6e4056-2243-4539-8126-22f357bbcd14",
   "metadata": {},
   "source": [
    "# Task 3: Understand human gesture and body language based on your own built dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6839b1-5947-4fbb-a59c-ec9115a8e650",
   "metadata": {},
   "source": [
    "## 1. Do literature search on dataset building and other deep learning based models applied on gesture recognition. Comment on their applications and benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254603d7-3f27-4e9e-9d29-7df04c362bda",
   "metadata": {},
   "source": [
    "## 2. In the earlier two tasks, you have learned how to do the gesture classification task using the given dataset. Now, you need to collect data by yourself and build your own dataset. The dataset is not limited to gestures. Postures and behavior are encouraged. Please place your data referring to the format of the given dataset. For good performance, the number of data in each class is recommended over 50. For the number of classes, it is better to have more than 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d1de2-a7ef-464a-8386-689961b7e594",
   "metadata": {},
   "source": [
    "## 3. Design your own neural network architecture. Fully connected or convolutional layers used in the first two tasks is acceptable. But you are encouraged to learn more deep learning models and achieve it as possible as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85ae6a-2551-4fd8-ba73-c6b7b3432081",
   "metadata": {},
   "source": [
    "## 3. Write down the problems you encountered during the experiment, the solutions, and your experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a684964-b989-41c9-978d-c892f6ddd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import torch.utils.data as utils_data\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.nn import Flatten, LogSoftmax, Linear, ReLU, LeakyReLU, CrossEntropyLoss, Sequential, Conv1d, Conv2d, MaxPool2d, Module, Softmax, BatchNorm1d, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a93d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'dataset',\n",
       " 'dataset.zip',\n",
       " 'Example.ipynb',\n",
       " 'Task 1 (3).ipynb',\n",
       " 'Task 2(anh).ipynb',\n",
       " 'Task 3.ipynb',\n",
       " 'task3_images',\n",
       " 'task3_images.zip',\n",
       " 'task3_model',\n",
       " 'trained_models']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get current path\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f029614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2939\n",
      "    Root location: ./task3_images\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(100, 100), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "#get path for images from Stanford40actions dataset\n",
    "path = './task3_images'\n",
    "#resize the dataset images to get a standard image size and convert to tensor\n",
    "dataset = ImageFolder(path,transform = transforms.Compose([\n",
    "    transforms.Resize((100,100)),transforms.ToTensor()\n",
    "]))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147b9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # code by yourself\n",
    "        # first CONV => RELU => POOL combos\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=6,\n",
    "        kernel_size=(5,5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        \n",
    "        # second CONV + LeakyRELU + POOL combos\n",
    "        self.conv2 = Conv2d(in_channels=6, out_channels=16,\n",
    "        kernel_size=(5,5))\n",
    "        self.lrelu2 = LeakyReLU(0.1)\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        \n",
    "        # third CONV + RELU + POOL combos\n",
    "        self.conv3 = Conv2d(in_channels=16, out_channels=32,\n",
    "        kernel_size=(3,3))\n",
    "        self.relu3 = ReLU()\n",
    "        self.maxpool3 = MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        \n",
    "        # fourth CONV + RELU + POOL combos\n",
    "        self.conv4 = Conv2d(in_channels=32, out_channels=64,\n",
    "        kernel_size=(3,3))\n",
    "        self.lrelu4 = LeakyReLU(0.1)\n",
    "        self.maxpool4 = MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        \n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=64*4*4, out_features=1024)\n",
    "        self.relu5 = ReLU()\n",
    "        self.fc2 = Linear(in_features=1024, out_features=256)\n",
    "        self.lrelu6 = LeakyReLU(0.1)\n",
    "        self.fc3 = Linear(in_features=256, out_features=84)\n",
    "        self.relu7 = ReLU()\n",
    "        self.fc4 = Linear(in_features=84, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        #conv layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        #last conv layer 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.lrelu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = x.view(-1,64*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu6(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu7(x)\n",
    "       \n",
    "        \n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        output = self.fc4(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773f14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes: applauding, blowing bubbles, brushing teeth,\n",
    "#drinking, holding umbrella, jumping, phoning, reading, \n",
    "#running, textinf, waving, writing\n",
    "model = CNNModel(num_classes=12)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"cuda activated\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "loss_func = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5532f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "#set training and testing data loaders\n",
    "dataset_size = len(dataset)\n",
    "split_ratio = 0.8 #80%training, 20%testing\n",
    "train_size = int(split_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_set, test_set = utils_data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = utils_data.DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n",
    "test_loader = utils_data.DataLoader(dataset=test_set, batch_size=16, shuffle=True)\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e49465",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\ttrain loss=0.094026\ttrain accuracy=0.984\ttest accuracy=0.231\n",
      "epoch=1\ttrain loss=0.127681\ttrain accuracy=0.965\ttest accuracy=0.240\n",
      "epoch=2\ttrain loss=0.281620\ttrain accuracy=0.918\ttest accuracy=0.233\n",
      "epoch=3\ttrain loss=0.188074\ttrain accuracy=0.946\ttest accuracy=0.228\n",
      "epoch=4\ttrain loss=0.149138\ttrain accuracy=0.957\ttest accuracy=0.231\n",
      "epoch=5\ttrain loss=0.052930\ttrain accuracy=0.994\ttest accuracy=0.235\n",
      "epoch=6\ttrain loss=0.038169\ttrain accuracy=0.997\ttest accuracy=0.241\n",
      "epoch=7\ttrain loss=0.024408\ttrain accuracy=0.999\ttest accuracy=0.228\n",
      "epoch=8\ttrain loss=0.019886\ttrain accuracy=0.999\ttest accuracy=0.233\n",
      "epoch=9\ttrain loss=0.017276\ttrain accuracy=1.000\ttest accuracy=0.233\n",
      "epoch=10\ttrain loss=0.014347\ttrain accuracy=1.000\ttest accuracy=0.238\n",
      "epoch=11\ttrain loss=0.012460\ttrain accuracy=1.000\ttest accuracy=0.226\n",
      "epoch=12\ttrain loss=0.010386\ttrain accuracy=1.000\ttest accuracy=0.248\n",
      "epoch=13\ttrain loss=0.008383\ttrain accuracy=1.000\ttest accuracy=0.247\n",
      "epoch=14\ttrain loss=0.007295\ttrain accuracy=1.000\ttest accuracy=0.233\n",
      "epoch=15\ttrain loss=0.006342\ttrain accuracy=1.000\ttest accuracy=0.235\n",
      "epoch=16\ttrain loss=0.005443\ttrain accuracy=1.000\ttest accuracy=0.243\n",
      "epoch=17\ttrain loss=0.004752\ttrain accuracy=1.000\ttest accuracy=0.231\n",
      "epoch=18\ttrain loss=0.004136\ttrain accuracy=1.000\ttest accuracy=0.240\n",
      "epoch=19\ttrain loss=0.003706\ttrain accuracy=1.000\ttest accuracy=0.235\n",
      "epoch=20\ttrain loss=0.003522\ttrain accuracy=1.000\ttest accuracy=0.224\n",
      "epoch=21\ttrain loss=0.003142\ttrain accuracy=1.000\ttest accuracy=0.240\n",
      "epoch=22\ttrain loss=0.002680\ttrain accuracy=1.000\ttest accuracy=0.228\n",
      "epoch=23\ttrain loss=0.002342\ttrain accuracy=1.000\ttest accuracy=0.236\n",
      "epoch=24\ttrain loss=0.002154\ttrain accuracy=1.000\ttest accuracy=0.228\n",
      "epoch=25\ttrain loss=0.001881\ttrain accuracy=1.000\ttest accuracy=0.238\n",
      "epoch=26\ttrain loss=0.001700\ttrain accuracy=1.000\ttest accuracy=0.241\n",
      "epoch=27\ttrain loss=0.001588\ttrain accuracy=1.000\ttest accuracy=0.236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      5\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_image, batch_label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    248\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "#Unlike in task 2, since this model has a larger data size, no.of epochs redued to 100\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, (batch_image, batch_label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        if torch.cuda.is_available():\n",
    "             batch_image, batch_label = batch_image.cuda(), batch_label.cuda()\n",
    "        batch_output = model(batch_image)\n",
    "        batch_loss = loss_func(batch_output, batch_label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, train_predicted = torch.max(batch_output.data, 1)\n",
    "        train_acc += (train_predicted == batch_label).sum().item()\n",
    "\n",
    "    train_acc /= train_size\n",
    "    running_loss /= (step+1)\n",
    "\n",
    "    # ----------test----------\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for test_image, test_label in test_loader:\n",
    "        test_output = model(test_image)\n",
    "        _, predicted = torch.max(test_output.data, 1)\n",
    "        test_acc += (predicted == test_label).sum().item()\n",
    "    test_acc /= test_size\n",
    "\n",
    "    print('epoch={:d}\\ttrain loss={:.6f}\\ttrain accuracy={:.3f}\\ttest accuracy={:.3f}'.format(\n",
    "        epoch, running_loss, train_acc, test_acc))\n",
    "\n",
    "    if test_acc >= best_accuracy:\n",
    "        torch.save(model.state_dict(), './task3_model/CNN_for_task3_model.pkl')\n",
    "        best_accuracy = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b79996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22861f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd4ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
